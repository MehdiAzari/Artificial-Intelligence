{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "italian-substitute",
   "metadata": {},
   "source": [
    "# Machine Learning's \"Hello World\"!\n",
    "\n",
    "This is the computer assignment **#3** of the **Artificial Intelligence** undergraduate course at **Shahid Beheshti University**.\n",
    "\n",
    "In this notebook, we explore the basic and traditional machine learning algorithms and see how to implement predicative models powered by [Sickit-Learn](https://scikit-learn.org/stable/) library.\n",
    "\n",
    " \n",
    " **Before you start:** Please read the ***Submission*** section at the bottom of the notebook carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subject-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import the essential packages we need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-operation",
   "metadata": {},
   "source": [
    "# 1. Medical Insurance Cost Prediction\n",
    "\n",
    "Many factors that affect how much you pay for health insurance are not within your control. Nonetheless, it's good to have an understanding of what they are. Here are some factors that affect how much health insurance premiums cost:\n",
    "\n",
    "- **age**: age of primary beneficiary\n",
    "- **sex**: insurance contractor gender, female, male\n",
    "- **bmi**: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight using the ratio of height to weight, ideally 18.5 to 24.9\n",
    "- **smoker**: Smoking\n",
    "- **children**: Number of children covered by health insurance / Number of dependents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stuck-barrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (1338, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>female</td>\n",
       "      <td>27.83</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>8515.7587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>male</td>\n",
       "      <td>32.70</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>34472.8410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>female</td>\n",
       "      <td>29.26</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>6184.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>male</td>\n",
       "      <td>34.50</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>13822.8030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>female</td>\n",
       "      <td>24.10</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>2974.1260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex    bmi  children smoker     charges\n",
       "0   45  female  27.83         2     no   8515.7587\n",
       "1   24    male  32.70         0    yes  34472.8410\n",
       "2   34  female  29.26         3     no   6184.2994\n",
       "3   64    male  34.50         0     no  13822.8030\n",
       "4   27  female  24.10         0     no   2974.1260"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/insurance.csv')\n",
    "print(\"Shape of the dataset: {}\".format(df.shape))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-wednesday",
   "metadata": {},
   "source": [
    "There is a library named `pandas_profiling` in Python include a method named as `ProfileReport()` which generate a basic report on the input DataFrame. The report consist of the following:\n",
    "\n",
    "- DataFrame overview,\n",
    "- Each attribute on which DataFrame is defined,\n",
    "- Correlations between attributes (Pearson Correlation and Spearman Correlation), and\n",
    "- A sample of DataFrame.\n",
    "\n",
    "By running the cell below, a HTML file will be created next to the notebook in the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broadband-reply",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4325aedbfb17441bb1ee18747272ba66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6adf9e96bbe54a7cb0cd01f046d9d879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e977c1f39f714dae85bf0fbc738275b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122ae36ccd7e4823884f89967fb37118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas_profiling library\n",
    "import pandas_profiling as pp\n",
    "\n",
    "# forming ProfileReport and save\n",
    "# as a HTML file\n",
    "profile = pp.ProfileReport(df)\n",
    "profile.to_file(\"insurance_dataset_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-taste",
   "metadata": {},
   "source": [
    "## 1.1 Linear Regression\n",
    "\n",
    "**Linear Regression** is a machine learning algorithm based on **supervised learning**. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting.\n",
    "<img src=\"images/linear-regression.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-thanksgiving",
   "metadata": {},
   "source": [
    "In this problem, we want to create a linear regression model for the existing dataest. For simplicity, we only consider one of the features of the data `bmi` and the target `charges`. Also we are just interested in the people who smokes. So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "damaged-drinking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'charges')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df[df['smoker'] =='yes']['bmi'].values  # X now is a numpy array\n",
    "target = df[df['smoker'] =='yes']['charges'].values  # also target is a numpy array\n",
    "\n",
    "# plot the points\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x = X, y = target)\n",
    "plt.xlabel(\"bmi\")\n",
    "plt.ylabel(\"charges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-witch",
   "metadata": {},
   "source": [
    "## Q1. Implement `SimpleLinearRegression` class. (20 points)\n",
    "\n",
    "Fill the blank lines with the least possible codes. Note that redundant codes may lead to reduce your score.\n",
    "\n",
    "**Note**: Do not add any other functions to the class defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "changing-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "class SimpleLinearRegression():\n",
    "    def __init__(self):\n",
    "        self.intercept = 0.0\n",
    "        self.coeff = 0.0\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        # TODO: Thin function takes X_train: numpy.ndarray and y : numpy.ndarray\n",
    "        # and fit a linear line to the points with the least loss\n",
    "        # Use sklearn.linear_model.LinearRegression for this purpose\n",
    "        # At last this function must set the intercept and coefficient of the predicted line\n",
    "        \n",
    "        intercept = 0.0\n",
    "        coeff = 0.0\n",
    "        ############# Your code here ############\n",
    "            \n",
    "        #########################################\n",
    "        \n",
    "        self.intercept = intercept\n",
    "        self.coeff = coeff\n",
    "        \n",
    "        return coeff, intercept\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true, loss_fn = 'MSE'):\n",
    "        # TODO: Implement this function that takes y_pred and y_true\n",
    "        # as a 1-dimensional numpy array (n_samples,) and returns\n",
    "        # the loss using sklearn.metrics functions\n",
    "         \n",
    "        possible_loss_functions = ['MSE', 'MAE', 'R2_Score']\n",
    "        \n",
    "        loss = None\n",
    "        if loss_fn == 'MSE':\n",
    "            ############# Your code here ############\n",
    "            loss = 0.0\n",
    "            #########################################\n",
    "        elif loss_fn == 'MAE':\n",
    "            ############# Your code here ############\n",
    "            loss = 0.0\n",
    "            #########################################\n",
    "        elif loss_fn == 'R2_Score':\n",
    "            ############# Your code here ############\n",
    "            loss = 0.0\n",
    "            #########################################\n",
    "        else:\n",
    "            error_str = 'Loss function is either unknown or not implemented.'\n",
    "            raise NotImplementedError(error_str)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def normalize(self, X, normalization='Standardization'):\n",
    "        # TODO: Implement this function that takes X : numpy.ndarray\n",
    "        # as the input feature array and normalize it\n",
    "        # You can use sklearn.preprocessing normalizations functions too.\n",
    "        # NOTE: For test set, you must use the mean and std of train set (standardization)\n",
    "        # of the train set. (since test set has not seen so far)\n",
    "        possible_normalization = ['Standardization', 'MinMaxScaling']\n",
    "        \n",
    "        nomalaized_feat = None\n",
    "        if normalization == 'Standardization':\n",
    "            ############# Your code here ############\n",
    "            nomalaized_feat = None\n",
    "            #########################################\n",
    "        elif normalization == 'MinMaxScaling':\n",
    "            ############# Your code here ############\n",
    "            nomalaized_feat = None\n",
    "            #########################################\n",
    "        else:\n",
    "            error_str = 'Normalization type is either unknown or not implemented.'\n",
    "            raise NotImplementedError(error_str)\n",
    "        \n",
    "        return nomalaized_feat\n",
    "    \n",
    "    def prepare_dataset(self, X, y, test_size=0.2, random_state=42):   \n",
    "        # TODO: Implement this function that takes X : numpy.ndarray and y : numpy.ndarray\n",
    "        # and use sklearn.model_selection.train_test_split to split your data into test and train sets\n",
    "        X_train, y_train, X_test, y_test = None, None, None, None\n",
    "        ############# Your code here ############\n",
    "    \n",
    "        #########################################\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-bunch",
   "metadata": {},
   "source": [
    "## Q2. What are the coefficient and intercept of the trained linear model? (5 points)\n",
    "\n",
    "After completing the functions above, use them to report the final coefficient and intercept of the predicted model.\n",
    "\n",
    "**Note**: When implementing the `SimpleLinearRegression` class, notice that before training your model, normalize your input features for better convergence by completeing `normalize()` function. If you forget it, it may hurts your model consequently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "drawn-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "slr = SimpleLinearRegression()\n",
    "############# Your code here ############\n",
    "            \n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-chrome",
   "metadata": {},
   "source": [
    "## Q3. Plot the prediceted line and the data points. (5 points)\n",
    "\n",
    "- Use `plt.scatter` to indicate the data points (Blue points for Train set and Red points for Test set).\n",
    "- Plot the predicted line using the coefficient and intercept calculated in the previous question. (`plt.plot`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ambient-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Your code here ############\n",
    "            \n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-seattle",
   "metadata": {},
   "source": [
    "## Q4. Compute loss. (5 points)\n",
    "\n",
    "First you must search about the following loss functions and compare them. (By showing their benefits to each other).\n",
    "\n",
    "- **MSE**: Mean Squared Error\n",
    "- **MAE**: Mean Absolute Error\n",
    "- **R2-score**\n",
    "\n",
    "Then create a DataFrame similar to the table shown below:\n",
    "\n",
    "| loss_function | train_set | test_set | \n",
    "| --- | --- | --- |\n",
    "| MSE | ... | ... |\n",
    "| MAE | ... | ... |\n",
    "| R2-score | ... | ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Your code here ############\n",
    "            \n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-basics",
   "metadata": {},
   "source": [
    "## 1.2 Underfitting & Overfitting Issues\n",
    "\n",
    "A model is said to be a good machine learning model if it generalizes any new input data from the problem domain in a proper way. This helps us to make predictions in the future data, that the data model has never seen. Now, suppose we want to check how well our machine learning model learns and generalizes to the new data (Test set). For that, we have overfitting and underfitting, which are majorly responsible for the poor performances of the machine learning algorithms.\n",
    "\n",
    "- **Underfitting:** A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data. (It’s just like trying to fit undersized pants!) Underfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm **does not fit the data well enough**. It usually happens when we have fewer data to build an accurate model and also when we try to build a linear model with fewer non-linear data. In such cases, the rules of the machine learning model are too easy and flexible to be applied on such minimal data and therefore the model will probably make a lot of wrong predictions. Underfitting can be avoided by using more data and also reducing the features by feature selection. \n",
    "\n",
    "    Techniques to reduce underfitting: \n",
    "\n",
    "    1. Increase model complexity\n",
    "    2. Increase the number of features, performing feature engineering\n",
    "    3. Remove noise from the data.\n",
    "    4. Increase the number of epochs or increase the duration of training to get better results.\n",
    "    \n",
    "    \n",
    "- **Overfitting:** A statistical model is said to be overfitted when we train it with a lot of data (just like fitting ourselves in oversized pants!). When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. \n",
    "\n",
    "    Techniques to reduce overfitting: \n",
    "\n",
    "    1. Increase training data.\n",
    "    2. Reduce model complexity.\n",
    "    3. Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "    4. Ridge Regularization and Lasso Regularization\n",
    "    \n",
    "    In the image below you can see linear regression and polynomial regression models. In the linear model, the model has underfitted and thus it cannot be generalized on all the data point properly (Underfitting). So we can add polynomial features as well to increase the complexity of the model. Therefore, in the middle figure you can see better fittnes of the model on the data points by adding $\\{x^2, x^3, x^4\\}$ to our hypothesis space.\n",
    "Although in the right side figure you see less loss than others, but the model is overfitted. It cannot be generalized on the test set (unseen data). \n",
    "    \n",
    "<img src=\"images/underfitting_overfitting.png\" width=700 height=700 />\n",
    "\n",
    "\n",
    "**Note:** The image above is not related to the given `insurance.csv` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-smith",
   "metadata": {},
   "source": [
    "## Q5. Implement `PolynomialRegression` class. (15 points)\n",
    "\n",
    "Fill the blank lines with using the least possible codes. Note that redundant codes may lead to reduce your score.\n",
    "\n",
    "**Note**: Do not add any other functions to the class defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "everyday-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "class PolynomialRegression():\n",
    "    def __init__(self, degree=3):\n",
    "        self.degree = degree\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        # TODO: Thin function takes X_train: numpy.ndarray and y : numpy.ndarray\n",
    "        # and fit a linear line to the points with the least loss\n",
    "        # Use sklearn.linear_model.LinearRegression for this purpose\n",
    "        # At last this function must set the intercept and coefficient of the predicted line\n",
    "        \n",
    "        intercept = None\n",
    "        coeff = None\n",
    "        ############# Your code here ############\n",
    "            \n",
    "        #########################################\n",
    "        \n",
    "        return coeff, intercept\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true, loss_fn = 'MSE'):\n",
    "        # TODO: Implement this function that takes y_pred and y_true\n",
    "        # as a 1-dimensional numpy array (n_samples,) and returns\n",
    "        # the loss using sklearn.metrics functions\n",
    "         \n",
    "        possible_loss_functions = ['MSE', 'MAE', 'R2_Score']\n",
    "        \n",
    "        loss = None\n",
    "        if loss_fn == 'MSE':\n",
    "            ############# Your code here ############\n",
    "            loss = 0.0\n",
    "            #########################################\n",
    "        elif loss_fn == 'MAE':\n",
    "            ############# Your code here ############\n",
    "            loss = 0.0\n",
    "            #########################################\n",
    "        elif loss_fn == 'R2_Score':\n",
    "            ############# Your code here ############\n",
    "            loss = 0.0\n",
    "            #########################################\n",
    "        else:\n",
    "            error_str = 'Loss function is either unknown or not implemented.'\n",
    "            raise NotImplementedError(error_str)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def normalize(self, X, normalization='Standardization'):\n",
    "        # TODO: Implement this function that takes X : numpy.ndarray\n",
    "        # as the input feature array and normalize it\n",
    "        # You can use sklearn.preprocessing normalizations functions too.\n",
    "        # NOTE: For test set, you must use the mean and std of train set (standardization)\n",
    "        # of the train set. (since test set has not seen so far)\n",
    "        possible_normalization = ['Standardization', 'MinMaxScaling']\n",
    "        \n",
    "        nomalaized_feat = None\n",
    "        if normalization == 'Standardization':\n",
    "            ############# Your code here ############\n",
    "            nomalaized_feat = None\n",
    "            #########################################\n",
    "        elif normalization == 'MinMaxScaling':\n",
    "            ############# Your code here ############\n",
    "            nomalaized_feat = None\n",
    "            #########################################\n",
    "        else:\n",
    "            error_str = 'Normalization type is either unknown or not implemented.'\n",
    "            raise NotImplementedError(error_str)\n",
    "        \n",
    "        return nomalaized_feat\n",
    "    \n",
    "    def generate_polynomial_features(self, X, degree):\n",
    "        # TODO: Implement this function that takes degree: int and X: numpy.ndarray\n",
    "        # to return polnomial features of those input features.\n",
    "        # Use sklearn.preprocessing.PolynomialFeatures. Read its documentation for more info.\n",
    "        X_pol = None\n",
    "        ############# Your code here ############\n",
    "    \n",
    "        #########################################\n",
    "        \n",
    "        return X_pol\n",
    "        \n",
    "\n",
    "    def prepare_dataset(self, X, y, test_size=0.2, random_state=42):   \n",
    "        # TODO: Implement this function that takes X : numpy.ndarray and y : numpy.ndarray\n",
    "        # and use sklearn.model_selection.train_test_split to split your data into test and train sets\n",
    "        X_train, y_train, X_test, y_test = None, None, None, None\n",
    "        ############# Your code here ############\n",
    "    \n",
    "        #########################################\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-living",
   "metadata": {},
   "source": [
    "## Q6. Find the best polynomial degree. (10 points)\n",
    "\n",
    "The best polynomial degree is the one with the least loss for test set data points. So use the implemented `PolynomialRegression`above multiple times to find the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "isolated-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "for degree in range(20):\n",
    "    best_degree = 1\n",
    "    least_loss = 0.0\n",
    "    pr = PolynomialRegression(degree=degree)\n",
    "    ############# Your code here ############\n",
    "    \n",
    "    #########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-personal",
   "metadata": {},
   "source": [
    "## Q7. Plot some experimental results (5 points)\n",
    "\n",
    "Use `plt.subplot` to plot the predicted polynomial model **for degrees, 1, 4 and 20**.\n",
    "\n",
    "So note that you must present 1 figure with 3 plots.\n",
    "\n",
    "Also consider these:\n",
    "- Set the title of each plot to ***Polynomial Fit degree i***\n",
    "- Print loss of each model on both train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Your code here ############\n",
    "    \n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-guidance",
   "metadata": {},
   "source": [
    "# 2. Sign Language Classification\n",
    "\n",
    "The American Sign Language MNIST Dataset we are going to use is obtained from [Kaggle](https://www.kaggle.com/datamunge/sign-language-mnist). This dataset is much like the original MNIST dataset. Each training and test case consists of a numerical label (0–25) with a one-to-one correspondence to the English alphabet (0 corresponds to A) and a grayscale 28x28 pixel image with values ranging from 0–255. However, there is no label correspondence to the letter J (9) and Z (25) due to the motion required to symbolize those letters. The number of testing and training cases in this dataset are much lower compared to the orginal MNIST dataset since there are only 27,455 training cases and 7,172 tests cases in this dataset.\n",
    "\n",
    "<img src=\"images/sign_language.png\" width=700 height=700 />\n",
    "\n",
    "In this problem we are going to implement a simple multi-class classification model to classify each image to its desired label. For downloading the dataset, use this [link](https://www.kaggle.com/datamunge/sign-language-mnist).\n",
    "\n",
    "**Note:** You may need to create a new account in Kaggle if you have not registered yet. Then put the downloaded directory in the `dataset/` folder with the name of : `sign-dataset/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "substantial-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "train_dataset = pd.read_csv('datasets/sign-dataset/sign_mnist_train/sign_mnist_train.csv')\n",
    "test_dataset = pd.read_csv('datasets/sign-dataset/sign_mnist_test/sign_mnist_test.csv')\n",
    "num_rows = train_dataset.shape[0]\n",
    "# To map each label number to its corresponding letter\n",
    "letters = dict(enumerate(string.ascii_uppercase))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-crack",
   "metadata": {},
   "source": [
    "Now it is the time foe data exploration! The first few rows of the training datast are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gorgeous-buffer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>161</td>\n",
       "      <td>168</td>\n",
       "      <td>172</td>\n",
       "      <td>173</td>\n",
       "      <td>178</td>\n",
       "      <td>184</td>\n",
       "      <td>189</td>\n",
       "      <td>193</td>\n",
       "      <td>196</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>74</td>\n",
       "      <td>68</td>\n",
       "      <td>62</td>\n",
       "      <td>53</td>\n",
       "      <td>55</td>\n",
       "      <td>48</td>\n",
       "      <td>238</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>136</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>102</td>\n",
       "      <td>91</td>\n",
       "      <td>65</td>\n",
       "      <td>138</td>\n",
       "      <td>189</td>\n",
       "      <td>179</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>114</td>\n",
       "      <td>42</td>\n",
       "      <td>74</td>\n",
       "      <td>99</td>\n",
       "      <td>104</td>\n",
       "      <td>109</td>\n",
       "      <td>117</td>\n",
       "      <td>127</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>214</td>\n",
       "      <td>218</td>\n",
       "      <td>220</td>\n",
       "      <td>223</td>\n",
       "      <td>223</td>\n",
       "      <td>225</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>228</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>169</td>\n",
       "      <td>174</td>\n",
       "      <td>176</td>\n",
       "      <td>180</td>\n",
       "      <td>183</td>\n",
       "      <td>185</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>190</td>\n",
       "      <td>...</td>\n",
       "      <td>119</td>\n",
       "      <td>118</td>\n",
       "      <td>123</td>\n",
       "      <td>120</td>\n",
       "      <td>118</td>\n",
       "      <td>114</td>\n",
       "      <td>94</td>\n",
       "      <td>74</td>\n",
       "      <td>61</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>191</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>53</td>\n",
       "      <td>200</td>\n",
       "      <td>204</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>193</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     13     164     167     170     172     176     179     180     184   \n",
       "5     16     161     168     172     173     178     184     189     193   \n",
       "6      8     134     134     135     135     136     137     137     138   \n",
       "7     22     114      42      74      99     104     109     117     127   \n",
       "8      3     169     174     176     180     183     185     187     188   \n",
       "9      3     189     189     189     190     190     191     190     190   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "5     196  ...        76        74        68        62        53        55   \n",
       "6     138  ...       109       102        91        65       138       189   \n",
       "7     142  ...       214       218       220       223       223       225   \n",
       "8     190  ...       119       118       123       120       118       114   \n",
       "9     190  ...        13        53       200       204       201       201   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "5        48       238       255       255  \n",
       "6       179       181       181       179  \n",
       "7       227       227       228       228  \n",
       "8        94        74        61        57  \n",
       "9       193       175       178       156  \n",
       "\n",
       "[10 rows x 785 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-topic",
   "metadata": {},
   "source": [
    "We need to separate the pixel values and the label from each other in order for us to load and access it separately. A function was constructed to split the training and testing dataset to separate the labels from the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "authorized-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_array(dataframe):\n",
    "    # Make a copy of the original dataframe\n",
    "    dataframe1 = dataframe.copy(deep=True)\n",
    "    # Extract input & outupts as numpy arrays\n",
    "    inputs_array = dataframe1.iloc[:, 1:].to_numpy()\n",
    "    targets_array = dataframe1['label'].to_numpy()\n",
    "    return inputs_array, targets_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "impressed-kingdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 784) (27455,)\n",
      "(7172, 784) (7172,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = dataframe_to_array(train_dataset)\n",
    "X_test, y_test = dataframe_to_array(test_dataset)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-onion",
   "metadata": {},
   "source": [
    "Let’s look at the first row of the training dataset. We also need to reshape the array to (28x28) since the initial shape is just a row array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "green-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letter:  D\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pic1 = np.reshape(X_train[0], (28, 28))\n",
    "plt.imshow(pic1, cmap = \"gray\")\n",
    "print(\"Letter: \", letters[y_train[0].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-purchase",
   "metadata": {},
   "source": [
    "As expected, the letter in the hand image is D. However, it is evident that the image is not clear due to its small resolution. This may affect the accuracy of our model and the implementation of the model in a much larger scale.\n",
    "\n",
    "The training and testing input arrays are converted to continuous float values since it allows our model for a more precise learning as compared to discrete values. On the other hand, the training and testing labels are converted to long integers since the output of the model are indices to be used in accessing probability values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-chase",
   "metadata": {},
   "source": [
    "## Q8. Implement `SimpleLogisticRegression` class. (25 points)\n",
    "\n",
    "You are free to search on the internet about implementing a Logistic Regression model using `sklearn.linear_model.LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "forced-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "############# Your code here ############\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-destruction",
   "metadata": {},
   "source": [
    "## Q9. Multi-label classification metrics. (10 points)\n",
    "\n",
    "Search on the web and find **two** best metrics for multi-label classification. Write about how it works and why we use them. Then, implement it using `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Your code here ############\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-costa",
   "metadata": {},
   "source": [
    "# 3. Submission\n",
    "\n",
    "Please read the notes here carefully:\n",
    "\n",
    "1. The more beautiful and insightfull your plots and diagrams are, the more points you get. So please take your time and concentration to prepare a good report with nice diagrams.\n",
    "\n",
    "2. The file you upload must be named as `[Student ID]-[Your name].zip` and it must contain **only 1 file**:\n",
    "\n",
    "  - `Linear_and_Logistic_Regression.ipynb`\n",
    "  \n",
    "4. **Important Note**: The outputs of the code blocks must be remained in your notebook, otherwise, you definitly lose all the points of that \n",
    "\n",
    "\n",
    "  \n",
    "In case you have any questions, contact **mohammad99hashemi@gmail.com**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
